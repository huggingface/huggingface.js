
# Inference code generated from the JSON schema spec in @huggingface/tasks.
#
# See:
#   - script: https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-codegen.ts
#   - specs:  https://github.com/huggingface/huggingface.js/tree/main/packages/tasks/src/tasks.

from typing import Literal, Optional, Dict, Any, List, Union
from dataclasses import dataclass


AudioFormat = Literal["wav", "mp3", "flac", "opus", "pcm16"]


@dataclass
class ChatCompletionInputAudio:
    """Parameters for audio output. Required when audio output is requested with
    `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
    """
    format: 'AudioFormat'
    """Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
    `opus`, or `pcm16`.
    """
    voice: str
    """The voice the model uses to respond. Supported voices are
    `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, and `shimmer`.
    """


@dataclass
class ChatCompletionInputFunctionCallOption:
    """Specifying a particular function via `{"name": "my_function"}` forces the model to call
    that function.
    """
    name: str
    """The name of the function to call."""


FunctionCallEnum = Literal["none", "auto"]


@dataclass
class ChatCompletionInputFunctions:
    name: str
    """The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
    dashes, with a maximum length of 64.
    """
    description: Optional[str]
    """A description of what the function does, used by the model to choose when and how to call
    the function.
    """
    parameters: Optional[Dict[str, Any]]


@dataclass
class ChatCompletionInputRequestAssistantMessageAudio:
    """Data about a previous audio response from the model.
    [Learn more](/docs/guides/audio).
    """
    id: str
    """Unique identifier for a previous audio response from the model."""


@dataclass
class ChatCompletionInputRequestMessageContentPartFileFile:
    file_data: Optional[str]
    """The base64 encoded file data, used when passing the file to the model
    as a string.
    """
    file_id: Optional[str]
    """The ID of an uploaded file to use as input."""
    filename: Optional[str]
    """The name of the file, used when passing the file to the model as a
    string.
    """


Detail = Literal["auto", "low", "high"]


@dataclass
class ChatCompletionInputRequestMessageContentPartImageImageURL:
    url: str
    """Either a URL of the image or the base64 encoded image data."""
    detail: Optional['Detail']
    """Specifies the detail level of the image. Learn more in the [Vision
    guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
    """


InputAudioFormat = Literal["wav", "mp3"]


@dataclass
class ChatCompletionInputRequestMessageContentPartAudioInputAudio:
    data: str
    """Base64 encoded audio data."""
    format: 'InputAudioFormat'
    """The format of the encoded audio data. Currently supports "wav" and "mp3"."""


PurpleType = Literal["text", "image_url", "input_audio", "file", "refusal"]


@dataclass
class ChatCompletionInputRequestMessageContentPart:
    """An array of content parts with a defined type. For developer messages, only type `text`
    is supported.
    
    Learn about [text inputs](/docs/guides/text-generation).
    
    
    An array of content parts with a defined type. Supported options differ based on the
    [model](/docs/models) being used to generate the response. Can contain text inputs.
    
    An array of content parts with a defined type. For system messages, only type `text` is
    supported.
    
    An array of content parts with a defined type. For tool messages, only type `text` is
    supported.
    
    An array of content parts with a defined type. Supported options differ based on the
    [model](/docs/models) being used to generate the response. Can contain text, image, or
    audio inputs.
    
    Learn about [image inputs](/docs/guides/vision).
    
    
    Learn about [audio inputs](/docs/guides/audio).
    
    
    Learn about [file inputs](/docs/guides/text) for text generation.
    
    
    An array of content parts with a defined type. Can be one or more of type `text`, or
    exactly one of type `refusal`.
    """
    type: 'PurpleType'
    """The type of the content part.
    
    The type of the content part. Always `input_audio`.
    
    The type of the content part. Always `file`.
    """
    file: Optional[ChatCompletionInputRequestMessageContentPartFileFile]
    image_url: Optional[ChatCompletionInputRequestMessageContentPartImageImageURL]
    input_audio: Optional[ChatCompletionInputRequestMessageContentPartAudioInputAudio]
    refusal: Optional[str]
    """The refusal message generated by the model."""
    text: Optional[str]
    """The text content."""


@dataclass
class ChatCompletionInputRequestAssistantMessageFunctionCall:
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    arguments: str
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: str
    """The name of the function to call."""


ChatCompletionInputRequestMessageRole = Literal["developer", "system", "user", "assistant", "tool", "function"]


@dataclass
class ChatCompletionInputMessageToolCallFunction:
    """The function that the model called."""
    arguments: str
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: str
    """The name of the function to call."""


ToolCallType = Literal["function"]


@dataclass
class ChatCompletionInputMessageToolCall:
    """The tool calls generated by the model, such as function calls."""
    function: ChatCompletionInputMessageToolCallFunction
    """The function that the model called."""
    id: str
    """The ID of the tool call."""
    type: 'ToolCallType'
    """The type of the tool. Currently, only `function` is supported."""


@dataclass
class ChatCompletionInputRequestMessage:
    """Developer-provided instructions that the model should follow, regardless of
    messages sent by the user. With o1 models and newer, `developer` messages
    replace the previous `system` messages.
    
    
    Developer-provided instructions that the model should follow, regardless of
    messages sent by the user. With o1 models and newer, use `developer` messages
    for this purpose instead.
    
    
    Messages sent by an end user, containing prompts or additional context
    information.
    
    
    Messages sent by the model in response to user messages.
    """
    role: 'ChatCompletionInputRequestMessageRole'
    """The role of the messages author, in this case `developer`.
    
    The role of the messages author, in this case `system`.
    
    The role of the messages author, in this case `user`.
    
    The role of the messages author, in this case `assistant`.
    
    The role of the messages author, in this case `tool`.
    
    The role of the messages author, in this case `function`.
    """
    audio: Optional[ChatCompletionInputRequestAssistantMessageAudio]
    """Data about a previous audio response from the model.
    [Learn more](/docs/guides/audio).
    """
    content: Optional[Union[List[ChatCompletionInputRequestMessageContentPart], str]]
    """The contents of the developer message.
    
    The contents of the system message.
    
    The contents of the user message.
    
    
    The contents of the assistant message. Required unless `tool_calls` or `function_call` is
    specified.
    
    
    The contents of the tool message.
    
    The contents of the function message.
    """
    function_call: Optional[ChatCompletionInputRequestAssistantMessageFunctionCall]
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    name: Optional[str]
    """An optional name for the participant. Provides the model information to differentiate
    between participants of the same role.
    
    The name of the function to call.
    """
    refusal: Optional[str]
    """The refusal message by the assistant."""
    tool_call_id: Optional[str]
    """Tool call that this message is responding to."""
    tool_calls: Optional[List[ChatCompletionInputMessageToolCall]]


ChatCompletionInputResponseModality = Literal["text", "audio"]


FluffyType = Literal["text"]


@dataclass
class ChatCompletionInputRequest:
    """An array of content parts with a defined type. For developer messages, only type `text`
    is supported.
    
    Learn about [text inputs](/docs/guides/text-generation).
    
    
    An array of content parts with a defined type. Supported options differ based on the
    [model](/docs/models) being used to generate the response. Can contain text inputs.
    
    An array of content parts with a defined type. For system messages, only type `text` is
    supported.
    
    An array of content parts with a defined type. For tool messages, only type `text` is
    supported.
    """
    text: str
    """The text content."""
    type: 'FluffyType'
    """The type of the content part."""


PredictionType = Literal["content"]


@dataclass
class ChatCompletionInputPredictionContent:
    """Configuration for a [Predicted Output](/docs/guides/predicted-outputs),
    which can greatly improve response times when large parts of the model
    response are known ahead of time. This is most common when you are
    regenerating a file with only minor changes to most of the content.
    
    
    Static predicted output content, such as the content of a text file that is
    being regenerated.
    """
    content: Union[List[ChatCompletionInputRequest], str]
    """The content that should be matched when generating a model response.
    If generated tokens would match this content, the entire model response
    can be returned much more quickly.
    """
    type: 'PredictionType'
    """The type of the predicted content you want to provide. This type is
    currently always `content`.
    """


ReasoningEffortEnum = Literal["low", "medium", "high"]


@dataclass
class JSONSchema:
    """Structured Outputs configuration options, including a JSON Schema."""
    name: str
    """The name of the response format. Must be a-z, A-Z, 0-9, or contain
    underscores and dashes, with a maximum length of 64.
    """
    description: Optional[str]
    """A description of what the response format is for, used by the model to
    determine how to respond in the format.
    """
    schema: Optional[Dict[str, Any]]
    strict: Optional[bool]
    """Whether to enable strict schema adherence when generating the output.
    If set to true, the model will always follow the exact schema defined
    in the `schema` field. Only a subset of JSON Schema is supported when
    `strict` is `true`. To learn more, read the [Structured Outputs
    guide](/docs/guides/structured-outputs).
    """


ResponseFormatType = Literal["text", "json_schema", "json_object"]


@dataclass
class ChatCompletionInputResponseFormat:
    """An object specifying the format that the model must output.
    
    Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
    Structured Outputs which ensures the model will match your supplied JSON
    schema. Learn more in the [Structured Outputs
    guide](/docs/guides/structured-outputs).
    
    Setting to `{ "type": "json_object" }` enables the older JSON mode, which
    ensures the message the model generates is valid JSON. Using `json_schema`
    is preferred for models that support it.
    
    
    Default response format. Used to generate text responses.
    
    
    JSON Schema response format. Used to generate structured JSON responses.
    Learn more about [Structured Outputs](/docs/guides/structured-outputs).
    
    
    JSON object response format. An older method of generating JSON responses.
    Using `json_schema` is recommended for models that support it. Note that the
    model will not generate JSON without a system or user message instructing it
    to do so.
    """
    type: 'ResponseFormatType'
    """The type of response format being defined. Always `text`.
    
    The type of response format being defined. Always `json_schema`.
    
    The type of response format being defined. Always `json_object`.
    """
    json_schema: Optional[JSONSchema]
    """Structured Outputs configuration options, including a JSON Schema."""


ChatCompletionInputServiceTier = Literal["auto", "default"]


@dataclass
class ChatCompletionInputStreamOptions:
    """Options for streaming response. Only set this when you set `stream: true`."""
    include_usage: Optional[bool]
    """If set, an additional chunk will be streamed before the `data: [DONE]`
    message. The `usage` field on this chunk shows the token usage statistics
    for the entire request, and the `choices` field will always be an empty
    array.
    
    All other chunks will also include a `usage` field, but with a null
    value. **NOTE:** If the stream is interrupted, you may not receive the
    final usage chunk which contains the total token usage for the request.
    """


@dataclass
class ChatCompletionInputNamedToolChoiceFunction:
    name: str
    """The name of the function to call."""


@dataclass
class ChatCompletionInputNamedToolChoice:
    """Specifies a tool the model should use. Use to force the model to call a specific function."""
    function: ChatCompletionInputNamedToolChoiceFunction
    type: 'ToolCallType'
    """The type of the tool. Currently, only `function` is supported."""


ChatCompletionInputToolChoiceOptionEnum = Literal["none", "auto", "required"]


@dataclass
class ChatCompletionInputFunctionObject:
    name: str
    """The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
    dashes, with a maximum length of 64.
    """
    description: Optional[str]
    """A description of what the function does, used by the model to choose when and how to call
    the function.
    """
    parameters: Optional[Dict[str, Any]]
    strict: Optional[bool]
    """Whether to enable strict schema adherence when generating the function call. If set to
    true, the model will follow the exact schema defined in the `parameters` field. Only a
    subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured
    Outputs in the [function calling guide](docs/guides/function-calling).
    """


@dataclass
class ChatCompletionInputTool:
    function: ChatCompletionInputFunctionObject
    type: 'ToolCallType'
    """The type of the tool. Currently, only `function` is supported."""


@dataclass
class ChatCompletionInputWebSearchLocation:
    """Approximate location parameters for the search."""
    city: Optional[str]
    """Free text input for the city of the user, e.g. `San Francisco`."""
    country: Optional[str]
    """The two-letter
    [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user,
    e.g. `US`.
    """
    region: Optional[str]
    """Free text input for the region of the user, e.g. `California`."""
    timezone: Optional[str]
    """The [IANA timezone](https://timeapi.io/documentation/iana-timezones)
    of the user, e.g. `America/Los_Angeles`.
    """


UserLocationType = Literal["approximate"]


@dataclass
class ChatCompletionInputUserLocation:
    """Approximate location parameters for the search."""
    approximate: ChatCompletionInputWebSearchLocation
    type: 'UserLocationType'
    """The type of location approximation. Always `approximate`."""


@dataclass
class WebSearch:
    """This tool searches the web for relevant results to use in a response.
    Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
    """
    search_context_size: Optional['ReasoningEffortEnum']
    user_location: Optional[ChatCompletionInputUserLocation]
    """Approximate location parameters for the search."""


@dataclass
class ChatCompletionInput:
    """Chat Completion Input.
    
    Auto-generated from OAI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
    """
    messages: List[ChatCompletionInputRequestMessage]
    """A list of messages comprising the conversation so far. Depending on the
    [model](/docs/models) you use, different message types (modalities) are
    supported, like [text](/docs/guides/text-generation),
    [images](/docs/guides/vision), and [audio](/docs/guides/audio).
    """
    model: str
    """Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI
    offers a wide range of models with different capabilities, performance
    characteristics, and price points. Refer to the [model guide](/docs/models)
    to browse and compare available models.
    """
    audio: Optional[ChatCompletionInputAudio]
    """Parameters for audio output. Required when audio output is requested with
    `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
    """
    frequency_penalty: Optional[float]
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on
    their existing frequency in the text so far, decreasing the model's
    likelihood to repeat the same line verbatim.
    """
    function_call: Optional[Union[ChatCompletionInputFunctionCallOption, 'FunctionCallEnum']]
    """Deprecated in favor of `tool_choice`.
    
    Controls which (if any) function is called by the model.
    
    `none` means the model will not call a function and instead generates a
    message.
    
    `auto` means the model can pick between generating a message or calling a
    function.
    
    Specifying a particular function via `{"name": "my_function"}` forces the
    model to call that function.
    
    `none` is the default when no functions are present. `auto` is the default
    if functions are present.
    """
    functions: Optional[List[ChatCompletionInputFunctions]]
    """Deprecated in favor of `tools`.
    
    A list of functions the model may generate JSON inputs for.
    """
    logit_bias: Optional[Dict[str, int]]
    """Modify the likelihood of specified tokens appearing in the completion.
    
    Accepts a JSON object that maps tokens (specified by their token ID in the
    tokenizer) to an associated bias value from -100 to 100. Mathematically,
    the bias is added to the logits generated by the model prior to sampling.
    The exact effect will vary per model, but values between -1 and 1 should
    decrease or increase likelihood of selection; values like -100 or 100
    should result in a ban or exclusive selection of the relevant token.
    """
    logprobs: Optional[bool]
    """Whether to return log probabilities of the output tokens or not. If true,
    returns the log probabilities of each output token returned in the
    `content` of `message`.
    """
    max_completion_tokens: Optional[int]
    """An upper bound for the number of tokens that can be generated for a completion, including
    visible output tokens and [reasoning tokens](/docs/guides/reasoning).
    """
    max_tokens: Optional[int]
    """The maximum number of [tokens](/tokenizer) that can be generated in the
    chat completion. This value can be used to control
    [costs](https://openai.com/api/pricing/) for text generated via API.
    
    This value is now deprecated in favor of `max_completion_tokens`, and is
    not compatible with [o1 series models](/docs/guides/reasoning).
    """
    modalities: Optional[List['ChatCompletionInputResponseModality']]
    n: Optional[int]
    """How many chat completion choices to generate for each input message. Note that you will
    be charged based on the number of generated tokens across all of the choices. Keep `n` as
    `1` to minimize costs.
    """
    parallel_tool_calls: Optional[bool]
    prediction: Optional[ChatCompletionInputPredictionContent]
    """Configuration for a [Predicted Output](/docs/guides/predicted-outputs),
    which can greatly improve response times when large parts of the model
    response are known ahead of time. This is most common when you are
    regenerating a file with only minor changes to most of the content.
    """
    presence_penalty: Optional[float]
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far, increasing the model's likelihood
    to talk about new topics.
    """
    reasoning_effort: Optional['ReasoningEffortEnum']
    response_format: Optional[ChatCompletionInputResponseFormat]
    """An object specifying the format that the model must output.
    
    Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
    Structured Outputs which ensures the model will match your supplied JSON
    schema. Learn more in the [Structured Outputs
    guide](/docs/guides/structured-outputs).
    
    Setting to `{ "type": "json_object" }` enables the older JSON mode, which
    ensures the message the model generates is valid JSON. Using `json_schema`
    is preferred for models that support it.
    """
    seed: Optional[int]
    """This feature is in Beta.
    If specified, our system will make a best effort to sample deterministically, such that
    repeated requests with the same `seed` and parameters should return the same result.
    Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
    parameter to monitor changes in the backend.
    """
    service_tier: Optional['ChatCompletionInputServiceTier']
    """Specifies the latency tier to use for processing the request. This parameter is relevant
    for customers subscribed to the scale tier service:
    - If set to 'auto', and the Project is Scale tier enabled, the system
    will utilize scale tier credits until they are exhausted.
    - If set to 'auto', and the Project is not Scale tier enabled, the request will be
    processed using the default service tier with a lower uptime SLA and no latency
    guarentee.
    - If set to 'default', the request will be processed using the default service tier with
    a lower uptime SLA and no latency guarentee.
    - When not set, the default behavior is 'auto'.
    
    When this parameter is set, the response body will include the `service_tier` utilized.
    """
    stop: Optional[Union[List[str], str]]
    store: Optional[bool]
    """Whether or not to store the output of this chat completion request for
    use in our [model distillation](/docs/guides/distillation) or
    [evals](/docs/guides/evals) products.
    """
    stream: Optional[bool]
    """If set to true, the model response data will be streamed to the client
    as it is generated using [server-sent
    events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
    See the [Streaming section below](/docs/api-reference/chat/streaming)
    for more information, along with the [streaming
    responses](/docs/guides/streaming-responses)
    guide for more information on how to handle the streaming events.
    """
    stream_options: Optional[ChatCompletionInputStreamOptions]
    tool_choice: Optional[Union[ChatCompletionInputNamedToolChoice, 'ChatCompletionInputToolChoiceOptionEnum']]
    tools: Optional[List[ChatCompletionInputTool]]
    """A list of tools the model may call. Currently, only functions are supported as a tool.
    Use this to provide a list of functions the model may generate JSON inputs for. A max of
    128 functions are supported.
    """
    top_logprobs: Optional[int]
    """An integer between 0 and 20 specifying the number of most likely tokens to
    return at each token position, each with an associated log probability.
    `logprobs` must be set to `true` if this parameter is used.
    """
    web_search_options: Optional[WebSearch]
    """This tool searches the web for relevant results to use in a response.
    Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
    """


FinishReason = Literal["stop", "length", "tool_calls", "content_filter", "function_call"]


@dataclass
class ChatCompletionOutputTokenLogprobTopLogprobsItem:
    bytes: List[int]
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
    instances where characters are represented by multiple tokens and their byte
    representations must be combined to generate the correct text representation. Can be
    `null` if there is no bytes representation for the token.
    """
    logprob: float
    """The log probability of this token, if it is within the top 20 most likely tokens.
    Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    token: str
    """The token."""


@dataclass
class ChatCompletionOutputTokenLogprob:
    bytes: List[int]
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
    instances where characters are represented by multiple tokens and their byte
    representations must be combined to generate the correct text representation. Can be
    `null` if there is no bytes representation for the token.
    """
    logprob: float
    """The log probability of this token, if it is within the top 20 most likely tokens.
    Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    token: str
    """The token."""
    top_logprobs: List[ChatCompletionOutputTokenLogprobTopLogprobsItem]
    """List of the most likely tokens and their log probability, at this token position. In rare
    cases, there may be fewer than the number of requested `top_logprobs` returned.
    """


@dataclass
class ChatCompletionOutputLogprobs:
    """Log probability information for the choice."""
    content: List[ChatCompletionOutputTokenLogprob]
    """A list of message content tokens with log probability information."""
    refusal: List[ChatCompletionOutputTokenLogprob]
    """A list of message refusal tokens with log probability information."""


AnnotationType = Literal["url_citation"]


@dataclass
class ChatCompletionOutputResponseMessageURLCitation:
    """A URL citation when using web search."""
    end_index: int
    """The index of the last character of the URL citation in the message."""
    start_index: int
    """The index of the first character of the URL citation in the message."""
    title: str
    """The title of the web resource."""
    url: str
    """The URL of the web resource."""


@dataclass
class ChatCompletionOutputResponseMessageAnnotationsItem:
    """A URL citation when using web search."""
    type: 'AnnotationType'
    """The type of the URL citation. Always `url_citation`."""
    url_citation: ChatCompletionOutputResponseMessageURLCitation
    """A URL citation when using web search."""


@dataclass
class ChatCompletionOutputResponseMessageAudio:
    """If the audio output modality is requested, this object contains data
    about the audio response from the model. [Learn more](/docs/guides/audio).
    """
    data: str
    """Base64 encoded audio bytes generated by the model, in the format
    specified in the request.
    """
    expires_at: int
    """The Unix timestamp (in seconds) for when this audio response will
    no longer be accessible on the server for use in multi-turn
    conversations.
    """
    id: str
    """Unique identifier for this audio response."""
    transcript: str
    """Transcript of the audio generated by the model."""


@dataclass
class ChatCompletionOutputResponseMessageFunctionCall:
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    arguments: str
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: str
    """The name of the function to call."""


MessageRole = Literal["assistant"]


@dataclass
class ChatCompletionOutputMessageToolCallFunction:
    """The function that the model called."""
    arguments: str
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: str
    """The name of the function to call."""


@dataclass
class ChatCompletionOutputMessageToolCall:
    """The tool calls generated by the model, such as function calls."""
    function: ChatCompletionOutputMessageToolCallFunction
    """The function that the model called."""
    id: str
    """The ID of the tool call."""
    type: 'ToolCallType'
    """The type of the tool. Currently, only `function` is supported."""


@dataclass
class ChatCompletionOutputResponseMessage:
    """A chat completion message generated by the model."""
    content: str
    """The contents of the message."""
    refusal: str
    """The refusal message generated by the model."""
    role: 'MessageRole'
    """The role of the author of this message."""
    annotations: Optional[List[ChatCompletionOutputResponseMessageAnnotationsItem]]
    """Annotations for the message, when applicable, as when using the
    [web search tool](/docs/guides/tools-web-search?api-mode=chat).
    """
    audio: Optional[ChatCompletionOutputResponseMessageAudio]
    """If the audio output modality is requested, this object contains data
    about the audio response from the model. [Learn more](/docs/guides/audio).
    """
    function_call: Optional[ChatCompletionOutputResponseMessageFunctionCall]
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    tool_calls: Optional[List[ChatCompletionOutputMessageToolCall]]


@dataclass
class ChatCompletionOutputChoicesItem:
    finish_reason: 'FinishReason'
    """The reason the model stopped generating tokens. This will be `stop` if the model hit a
    natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
    called a function.
    """
    index: int
    """The index of the choice in the list of choices."""
    logprobs: ChatCompletionOutputLogprobs
    """Log probability information for the choice."""
    message: ChatCompletionOutputResponseMessage


ChatCompletionOutputObject = Literal["chat.completion"]


ChatCompletionOutputServiceTier = Literal["scale", "default"]


@dataclass
class ChatCompletionOutputCompletionUsageCompletionTokensDetails:
    """Breakdown of tokens used in a completion."""
    accepted_prediction_tokens: Optional[int]
    """When using Predicted Outputs, the number of tokens in the
    prediction that appeared in the completion.
    """
    audio_tokens: Optional[int]
    """Audio input tokens generated by the model."""
    reasoning_tokens: Optional[int]
    """Tokens generated by the model for reasoning."""
    rejected_prediction_tokens: Optional[int]
    """When using Predicted Outputs, the number of tokens in the
    prediction that did not appear in the completion. However, like
    reasoning tokens, these tokens are still counted in the total
    completion tokens for purposes of billing, output, and context window
    limits.
    """


@dataclass
class ChatCompletionOutputCompletionUsagePromptTokensDetails:
    """Breakdown of tokens used in the prompt."""
    audio_tokens: Optional[int]
    """Audio input tokens present in the prompt."""
    cached_tokens: Optional[int]
    """Cached tokens present in the prompt."""


@dataclass
class ChatCompletionOutputCompletionUsage:
    """Usage statistics for the completion request."""
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""
    completion_tokens_details: Optional[ChatCompletionOutputCompletionUsageCompletionTokensDetails]
    """Breakdown of tokens used in a completion."""
    prompt_tokens_details: Optional[ChatCompletionOutputCompletionUsagePromptTokensDetails]
    """Breakdown of tokens used in the prompt."""


@dataclass
class ChatCompletionOutput:
    """Chat Completion Output.
    
    Auto-generated from OAI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
    """
    choices: List[ChatCompletionOutputChoicesItem]
    """A list of chat completion choices. Can be more than one if `n` is greater than 1."""
    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""
    id: str
    """A unique identifier for the chat completion."""
    model: str
    """The model used for the chat completion."""
    object: 'ChatCompletionOutputObject'
    """The object type, which is always `chat.completion`."""
    service_tier: Optional['ChatCompletionOutputServiceTier']
    """The service tier used for processing the request."""
    system_fingerprint: Optional[str]
    """This fingerprint represents the backend configuration that the model runs with.
    
    Can be used in conjunction with the `seed` request parameter to understand when backend
    changes have been made that might impact determinism.
    """
    usage: Optional[ChatCompletionOutputCompletionUsage]


@dataclass
class ChatCompletionStreamOutputStreamResponseDeltaFunctionCall:
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    arguments: Optional[str]
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: Optional[str]
    """The name of the function to call."""


DeltaRole = Literal["developer", "system", "user", "assistant", "tool"]


@dataclass
class ChatCompletionStreamOutputMessageToolCallChunkFunction:
    arguments: Optional[str]
    """The arguments to call the function with, as generated by the model in JSON format. Note
    that the model does not always generate valid JSON, and may hallucinate parameters not
    defined by your function schema. Validate the arguments in your code before calling your
    function.
    """
    name: Optional[str]
    """The name of the function to call."""


@dataclass
class ChatCompletionStreamOutputMessageToolCallChunk:
    index: int
    function: Optional[ChatCompletionStreamOutputMessageToolCallChunkFunction]
    id: Optional[str]
    """The ID of the tool call."""
    type: Optional['ToolCallType']
    """The type of the tool. Currently, only `function` is supported."""


@dataclass
class ChatCompletionStreamOutputStreamResponseDelta:
    """A chat completion delta generated by streamed model responses."""
    content: Optional[str]
    """The contents of the chunk message."""
    function_call: Optional[ChatCompletionStreamOutputStreamResponseDeltaFunctionCall]
    """Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    be called, as generated by the model.
    """
    refusal: Optional[str]
    """The refusal message generated by the model."""
    role: Optional['DeltaRole']
    """The role of the author of this message."""
    tool_calls: Optional[List[ChatCompletionStreamOutputMessageToolCallChunk]]


@dataclass
class ChatCompletionStreamOutputTokenLogprobTopLogprobsItem:
    bytes: List[int]
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
    instances where characters are represented by multiple tokens and their byte
    representations must be combined to generate the correct text representation. Can be
    `null` if there is no bytes representation for the token.
    """
    logprob: float
    """The log probability of this token, if it is within the top 20 most likely tokens.
    Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    token: str
    """The token."""


@dataclass
class ChatCompletionStreamOutputTokenLogprob:
    bytes: List[int]
    """A list of integers representing the UTF-8 bytes representation of the token. Useful in
    instances where characters are represented by multiple tokens and their byte
    representations must be combined to generate the correct text representation. Can be
    `null` if there is no bytes representation for the token.
    """
    logprob: float
    """The log probability of this token, if it is within the top 20 most likely tokens.
    Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    token: str
    """The token."""
    top_logprobs: List[ChatCompletionStreamOutputTokenLogprobTopLogprobsItem]
    """List of the most likely tokens and their log probability, at this token position. In rare
    cases, there may be fewer than the number of requested `top_logprobs` returned.
    """


@dataclass
class ChatCompletionStreamOutputLogprobs:
    """Log probability information for the choice."""
    content: List[ChatCompletionStreamOutputTokenLogprob]
    """A list of message content tokens with log probability information."""
    refusal: List[ChatCompletionStreamOutputTokenLogprob]
    """A list of message refusal tokens with log probability information."""


@dataclass
class ChatCompletionStreamOutputChoicesItem:
    delta: ChatCompletionStreamOutputStreamResponseDelta
    finish_reason: 'FinishReason'
    """The reason the model stopped generating tokens. This will be `stop` if the model hit a
    natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
    called a function.
    """
    index: int
    """The index of the choice in the list of choices."""
    logprobs: Optional[ChatCompletionStreamOutputLogprobs]
    """Log probability information for the choice."""


ChatCompletionStreamOutputObject = Literal["chat.completion.chunk"]


@dataclass
class ChatCompletionStreamOutputCompletionUsageCompletionTokensDetails:
    """Breakdown of tokens used in a completion."""
    accepted_prediction_tokens: Optional[int]
    """When using Predicted Outputs, the number of tokens in the
    prediction that appeared in the completion.
    """
    audio_tokens: Optional[int]
    """Audio input tokens generated by the model."""
    reasoning_tokens: Optional[int]
    """Tokens generated by the model for reasoning."""
    rejected_prediction_tokens: Optional[int]
    """When using Predicted Outputs, the number of tokens in the
    prediction that did not appear in the completion. However, like
    reasoning tokens, these tokens are still counted in the total
    completion tokens for purposes of billing, output, and context window
    limits.
    """


@dataclass
class ChatCompletionStreamOutputCompletionUsagePromptTokensDetails:
    """Breakdown of tokens used in the prompt."""
    audio_tokens: Optional[int]
    """Audio input tokens present in the prompt."""
    cached_tokens: Optional[int]
    """Cached tokens present in the prompt."""


@dataclass
class ChatCompletionStreamOutputCompletionUsage:
    """An optional field that will only be present when you set
    `stream_options: {"include_usage": true}` in your request. When present, it
    contains a null value **except for the last chunk** which contains the
    token usage statistics for the entire request.
    
    **NOTE:** If the stream is interrupted or cancelled, you may not
    receive the final usage chunk which contains the total token usage for
    the request.
    
    
    Usage statistics for the completion request.
    """
    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""
    completion_tokens_details: Optional[ChatCompletionStreamOutputCompletionUsageCompletionTokensDetails]
    """Breakdown of tokens used in a completion."""
    prompt_tokens_details: Optional[ChatCompletionStreamOutputCompletionUsagePromptTokensDetails]
    """Breakdown of tokens used in the prompt."""


@dataclass
class ChatCompletionStreamOutput:
    """Chat Completion Stream Output.
    
    Auto-generated from OAI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
    """
    choices: List[ChatCompletionStreamOutputChoicesItem]
    """A list of chat completion choices. Can contain more than one elements if `n` is greater
    than 1. Can also be empty for the
    last chunk if you set `stream_options: {"include_usage": true}`.
    """
    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has
    the same timestamp.
    """
    id: str
    """A unique identifier for the chat completion. Each chunk has the same ID."""
    model: str
    """The model to generate the completion."""
    object: 'ChatCompletionStreamOutputObject'
    """The object type, which is always `chat.completion.chunk`."""
    service_tier: Optional['ChatCompletionOutputServiceTier']
    """The service tier used for processing the request."""
    system_fingerprint: Optional[str]
    """This fingerprint represents the backend configuration that the model runs with.
    Can be used in conjunction with the `seed` request parameter to understand when backend
    changes have been made that might impact determinism.
    """
    usage: Optional[ChatCompletionStreamOutputCompletionUsage]
    """An optional field that will only be present when you set
    `stream_options: {"include_usage": true}` in your request. When present, it
    contains a null value **except for the last chunk** which contains the
    token usage statistics for the entire request.
    
    **NOTE:** If the stream is interrupted or cancelled, you may not
    receive the final usage chunk which contains the total token usage for
    the request.
    """
