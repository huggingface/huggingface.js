import type { TaskDataCustom } from "../index.js";

const taskData: TaskDataCustom = {
	datasets: [
		{
			// TODO write proper description
			description: "Dataset from 12M image-text of Reddit",
			id: "red_caps",
		},
		{
			// TODO write proper description
			description: "Dataset from 3.3M images of Google",
			id: "datasets/conceptual_captions",
		},
	],
	demo: {
		inputs: [
			{
				filename: "savanna.jpg",
				type: "img",
			},
		],
		outputs: [
			{
				label: "Detailed description",
				content: "a herd of giraffes and zebras grazing in a field",
				type: "text",
			},
		],
	},
	metrics: [],
	models: [
		{
			description: "Strong OCR model.",
			id: "allenai/olmOCR-7B-0725",
		},
		{
			description: "Powerful image captioning model.",
			id: "fancyfeast/llama-joycaption-beta-one-hf-llava",
		},
	],
	spaces: [
		{
			description: "SVG generator app from images.",
			id: "multimodalart/OmniSVG-3B",
		},
		{
			description: "An application that converts documents to markdown.",
			id: "numind/NuMarkdown-8B-Thinking",
		},
		{
			description: "An application that can caption images.",
			id: "fancyfeast/joy-caption-beta-one",
		},
	],
	summary:
		"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.",
	widgetModels: ["Salesforce/blip-image-captioning-large"],
	youtubeId: "",
};

export default taskData;
