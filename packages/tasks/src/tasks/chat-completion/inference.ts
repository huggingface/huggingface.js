/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */
/**
 * Chat Completion Input.
 *
 * Auto-generated from OAI specs.
 * For more details, check out
 * https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
 */
export interface ChatCompletionInput {
	/**
	 * Parameters for audio output. Required when audio output is requested with
	 * `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
	 */
	audio?: ChatCompletionInputAudio;
	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on
	 * their existing frequency in the text so far, decreasing the model's
	 * likelihood to repeat the same line verbatim.
	 */
	frequency_penalty?: number;
	/**
	 * Deprecated in favor of `tool_choice`.
	 *
	 * Controls which (if any) function is called by the model.
	 *
	 * `none` means the model will not call a function and instead generates a
	 * message.
	 *
	 * `auto` means the model can pick between generating a message or calling a
	 * function.
	 *
	 * Specifying a particular function via `{"name": "my_function"}` forces the
	 * model to call that function.
	 *
	 * `none` is the default when no functions are present. `auto` is the default
	 * if functions are present.
	 */
	function_call?: FunctionCallUnion;
	/**
	 * Deprecated in favor of `tools`.
	 *
	 * A list of functions the model may generate JSON inputs for.
	 */
	functions?: ChatCompletionInputFunctions[];
	/**
	 * Modify the likelihood of specified tokens appearing in the completion.
	 *
	 * Accepts a JSON object that maps tokens (specified by their token ID in the
	 * tokenizer) to an associated bias value from -100 to 100. Mathematically,
	 * the bias is added to the logits generated by the model prior to sampling.
	 * The exact effect will vary per model, but values between -1 and 1 should
	 * decrease or increase likelihood of selection; values like -100 or 100
	 * should result in a ban or exclusive selection of the relevant token.
	 */
	logit_bias?: {
		[key: string]: number;
	};
	/**
	 * Whether to return log probabilities of the output tokens or not. If true,
	 * returns the log probabilities of each output token returned in the
	 * `content` of `message`.
	 */
	logprobs?: boolean;
	/**
	 * An upper bound for the number of tokens that can be generated for a completion, including
	 * visible output tokens and [reasoning tokens](/docs/guides/reasoning).
	 */
	max_completion_tokens?: number;
	/**
	 * The maximum number of [tokens](/tokenizer) that can be generated in the
	 * chat completion. This value can be used to control
	 * [costs](https://openai.com/api/pricing/) for text generated via API.
	 *
	 * This value is now deprecated in favor of `max_completion_tokens`, and is
	 * not compatible with [o1 series models](/docs/guides/reasoning).
	 */
	max_tokens?: number;
	/**
	 * A list of messages comprising the conversation so far. Depending on the
	 * [model](/docs/models) you use, different message types (modalities) are
	 * supported, like [text](/docs/guides/text-generation),
	 * [images](/docs/guides/vision), and [audio](/docs/guides/audio).
	 */
	messages: ChatCompletionInputRequestMessage[];
	modalities?: ChatCompletionInputResponseModality[];
	/**
	 * Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI
	 * offers a wide range of models with different capabilities, performance
	 * characteristics, and price points. Refer to the [model guide](/docs/models)
	 * to browse and compare available models.
	 */
	model: string;
	/**
	 * How many chat completion choices to generate for each input message. Note that you will
	 * be charged based on the number of generated tokens across all of the choices. Keep `n` as
	 * `1` to minimize costs.
	 */
	n?: number;
	parallel_tool_calls?: boolean;
	/**
	 * Configuration for a [Predicted Output](/docs/guides/predicted-outputs),
	 * which can greatly improve response times when large parts of the model
	 * response are known ahead of time. This is most common when you are
	 * regenerating a file with only minor changes to most of the content.
	 */
	prediction?: ChatCompletionInputPredictionContent;
	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on
	 * whether they appear in the text so far, increasing the model's likelihood
	 * to talk about new topics.
	 */
	presence_penalty?: number;
	reasoning_effort?: ReasoningEffortEnum;
	/**
	 * An object specifying the format that the model must output.
	 *
	 * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
	 * Structured Outputs which ensures the model will match your supplied JSON
	 * schema. Learn more in the [Structured Outputs
	 * guide](/docs/guides/structured-outputs).
	 *
	 * Setting to `{ "type": "json_object" }` enables the older JSON mode, which
	 * ensures the message the model generates is valid JSON. Using `json_schema`
	 * is preferred for models that support it.
	 */
	response_format?: ChatCompletionInputResponseFormat;
	/**
	 * This feature is in Beta.
	 * If specified, our system will make a best effort to sample deterministically, such that
	 * repeated requests with the same `seed` and parameters should return the same result.
	 * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
	 * parameter to monitor changes in the backend.
	 */
	seed?: number;
	/**
	 * Specifies the latency tier to use for processing the request. This parameter is relevant
	 * for customers subscribed to the scale tier service:
	 * - If set to 'auto', and the Project is Scale tier enabled, the system
	 * will utilize scale tier credits until they are exhausted.
	 * - If set to 'auto', and the Project is not Scale tier enabled, the request will be
	 * processed using the default service tier with a lower uptime SLA and no latency
	 * guarentee.
	 * - If set to 'default', the request will be processed using the default service tier with
	 * a lower uptime SLA and no latency guarentee.
	 * - When not set, the default behavior is 'auto'.
	 *
	 * When this parameter is set, the response body will include the `service_tier` utilized.
	 */
	service_tier?: ChatCompletionInputServiceTier;
	stop?: ChatCompletionInputStopConfiguration;
	/**
	 * Whether or not to store the output of this chat completion request for
	 * use in our [model distillation](/docs/guides/distillation) or
	 * [evals](/docs/guides/evals) products.
	 */
	store?: boolean;
	/**
	 * If set to true, the model response data will be streamed to the client
	 * as it is generated using [server-sent
	 * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
	 * See the [Streaming section below](/docs/api-reference/chat/streaming)
	 * for more information, along with the [streaming
	 * responses](/docs/guides/streaming-responses)
	 * guide for more information on how to handle the streaming events.
	 */
	stream?: boolean;
	stream_options?: ChatCompletionInputStreamOptions;
	tool_choice?: ChatCompletionInputToolChoiceOption;
	/**
	 * A list of tools the model may call. Currently, only functions are supported as a tool.
	 * Use this to provide a list of functions the model may generate JSON inputs for. A max of
	 * 128 functions are supported.
	 */
	tools?: ChatCompletionInputTool[];
	/**
	 * An integer between 0 and 20 specifying the number of most likely tokens to
	 * return at each token position, each with an associated log probability.
	 * `logprobs` must be set to `true` if this parameter is used.
	 */
	top_logprobs?: number;
	/**
	 * This tool searches the web for relevant results to use in a response.
	 * Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
	 */
	web_search_options?: WebSearch;
	[property: string]: unknown;
}
/**
 * Parameters for audio output. Required when audio output is requested with
 * `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
 */
export interface ChatCompletionInputAudio {
	/**
	 * Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
	 * `opus`, or `pcm16`.
	 */
	format: AudioFormat;
	/**
	 * The voice the model uses to respond. Supported voices are
	 * `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, and `shimmer`.
	 */
	voice: string;
	[property: string]: unknown;
}
/**
 * Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
 * `opus`, or `pcm16`.
 */
export type AudioFormat = "wav" | "mp3" | "flac" | "opus" | "pcm16";
/**
 * Deprecated in favor of `tool_choice`.
 *
 * Controls which (if any) function is called by the model.
 *
 * `none` means the model will not call a function and instead generates a
 * message.
 *
 * `auto` means the model can pick between generating a message or calling a
 * function.
 *
 * Specifying a particular function via `{"name": "my_function"}` forces the
 * model to call that function.
 *
 * `none` is the default when no functions are present. `auto` is the default
 * if functions are present.
 */
export type FunctionCallUnion = FunctionCallEnum | ChatCompletionInputFunctionCallOption;
/**
 * `none` means the model will not call a function and instead generates a message. `auto`
 * means the model can pick between generating a message or calling a function.
 */
export type FunctionCallEnum = "none" | "auto";
/**
 * Specifying a particular function via `{"name": "my_function"}` forces the model to call
 * that function.
 */
export interface ChatCompletionInputFunctionCallOption {
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
export interface ChatCompletionInputFunctions {
	/**
	 * A description of what the function does, used by the model to choose when and how to call
	 * the function.
	 */
	description?: string;
	/**
	 * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
	 * dashes, with a maximum length of 64.
	 */
	name: string;
	parameters?: {
		[key: string]: unknown;
	};
	[property: string]: unknown;
}
/**
 * Developer-provided instructions that the model should follow, regardless of
 * messages sent by the user. With o1 models and newer, `developer` messages
 * replace the previous `system` messages.
 *
 *
 * Developer-provided instructions that the model should follow, regardless of
 * messages sent by the user. With o1 models and newer, use `developer` messages
 * for this purpose instead.
 *
 *
 * Messages sent by an end user, containing prompts or additional context
 * information.
 *
 *
 * Messages sent by the model in response to user messages.
 */
export interface ChatCompletionInputRequestMessage {
	/**
	 * Data about a previous audio response from the model.
	 * [Learn more](/docs/guides/audio).
	 */
	audio?: ChatCompletionInputRequestAssistantMessageAudio;
	/**
	 * The contents of the developer message.
	 *
	 * The contents of the system message.
	 *
	 * The contents of the user message.
	 *
	 *
	 * The contents of the assistant message. Required unless `tool_calls` or `function_call` is
	 * specified.
	 *
	 *
	 * The contents of the tool message.
	 *
	 * The contents of the function message.
	 */
	content?: ChatCompletionInputRequestMessageContent;
	/**
	 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
	 * be called, as generated by the model.
	 */
	function_call?: ChatCompletionInputRequestAssistantMessageFunctionCall;
	/**
	 * An optional name for the participant. Provides the model information to differentiate
	 * between participants of the same role.
	 *
	 * The name of the function to call.
	 */
	name?: string;
	/**
	 * The refusal message by the assistant.
	 */
	refusal?: string;
	/**
	 * The role of the messages author, in this case `developer`.
	 *
	 * The role of the messages author, in this case `system`.
	 *
	 * The role of the messages author, in this case `user`.
	 *
	 * The role of the messages author, in this case `assistant`.
	 *
	 * The role of the messages author, in this case `tool`.
	 *
	 * The role of the messages author, in this case `function`.
	 */
	role: ChatCompletionInputRequestMessageRole;
	/**
	 * Tool call that this message is responding to.
	 */
	tool_call_id?: string;
	tool_calls?: ChatCompletionInputMessageToolCall[];
	[property: string]: unknown;
}
/**
 * Data about a previous audio response from the model.
 * [Learn more](/docs/guides/audio).
 */
export interface ChatCompletionInputRequestAssistantMessageAudio {
	/**
	 * Unique identifier for a previous audio response from the model.
	 */
	id: string;
	[property: string]: unknown;
}
export type ChatCompletionInputRequestMessageContent = ChatCompletionInputRequestMessageContentPart[] | string;
/**
 * An array of content parts with a defined type. For developer messages, only type `text`
 * is supported.
 *
 * Learn about [text inputs](/docs/guides/text-generation).
 *
 *
 * An array of content parts with a defined type. Supported options differ based on the
 * [model](/docs/models) being used to generate the response. Can contain text inputs.
 *
 * An array of content parts with a defined type. For system messages, only type `text` is
 * supported.
 *
 * An array of content parts with a defined type. For tool messages, only type `text` is
 * supported.
 *
 * An array of content parts with a defined type. Supported options differ based on the
 * [model](/docs/models) being used to generate the response. Can contain text, image, or
 * audio inputs.
 *
 * Learn about [image inputs](/docs/guides/vision).
 *
 *
 * Learn about [audio inputs](/docs/guides/audio).
 *
 *
 * Learn about [file inputs](/docs/guides/text) for text generation.
 *
 *
 * An array of content parts with a defined type. Can be one or more of type `text`, or
 * exactly one of type `refusal`.
 */
export interface ChatCompletionInputRequestMessageContentPart {
	file?: ChatCompletionInputRequestMessageContentPartFileFile;
	image_url?: ChatCompletionInputRequestMessageContentPartImageImageURL;
	input_audio?: ChatCompletionInputRequestMessageContentPartAudioInputAudio;
	/**
	 * The refusal message generated by the model.
	 */
	refusal?: string;
	/**
	 * The text content.
	 */
	text?: string;
	/**
	 * The type of the content part.
	 *
	 * The type of the content part. Always `input_audio`.
	 *
	 * The type of the content part. Always `file`.
	 */
	type: PurpleType;
	[property: string]: unknown;
}
export interface ChatCompletionInputRequestMessageContentPartFileFile {
	/**
	 * The base64 encoded file data, used when passing the file to the model
	 * as a string.
	 */
	file_data?: string;
	/**
	 * The ID of an uploaded file to use as input.
	 */
	file_id?: string;
	/**
	 * The name of the file, used when passing the file to the model as a
	 * string.
	 */
	filename?: string;
	[property: string]: unknown;
}
export interface ChatCompletionInputRequestMessageContentPartImageImageURL {
	/**
	 * Specifies the detail level of the image. Learn more in the [Vision
	 * guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
	 */
	detail?: Detail;
	/**
	 * Either a URL of the image or the base64 encoded image data.
	 */
	url: string;
	[property: string]: unknown;
}
/**
 * Specifies the detail level of the image. Learn more in the [Vision
 * guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
 */
export type Detail = "auto" | "low" | "high";
export interface ChatCompletionInputRequestMessageContentPartAudioInputAudio {
	/**
	 * Base64 encoded audio data.
	 */
	data: string;
	/**
	 * The format of the encoded audio data. Currently supports "wav" and "mp3".
	 */
	format: InputAudioFormat;
	[property: string]: unknown;
}
/**
 * The format of the encoded audio data. Currently supports "wav" and "mp3".
 */
export type InputAudioFormat = "wav" | "mp3";
/**
 * The type of the content part.
 *
 * The type of the content part. Always `input_audio`.
 *
 * The type of the content part. Always `file`.
 */
export type PurpleType = "text" | "image_url" | "input_audio" | "file" | "refusal";
/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
 * be called, as generated by the model.
 */
export interface ChatCompletionInputRequestAssistantMessageFunctionCall {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments: string;
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
/**
 * The role of the messages author, in this case `developer`.
 *
 * The role of the messages author, in this case `system`.
 *
 * The role of the messages author, in this case `user`.
 *
 * The role of the messages author, in this case `assistant`.
 *
 * The role of the messages author, in this case `tool`.
 *
 * The role of the messages author, in this case `function`.
 */
export type ChatCompletionInputRequestMessageRole = "developer" | "system" | "user" | "assistant" | "tool" | "function";
/**
 * The tool calls generated by the model, such as function calls.
 */
export interface ChatCompletionInputMessageToolCall {
	/**
	 * The function that the model called.
	 */
	function: ChatCompletionInputMessageToolCallFunction;
	/**
	 * The ID of the tool call.
	 */
	id: string;
	/**
	 * The type of the tool. Currently, only `function` is supported.
	 */
	type: "function";
	[property: string]: unknown;
}
/**
 * The function that the model called.
 */
export interface ChatCompletionInputMessageToolCallFunction {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments: string;
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
/**
 * The type of the tool. Currently, only `function` is supported.
 */
/**
 * Output types that you would like the model to generate.
 * Most models are capable of generating text, which is the default:
 *
 * `["text"]`
 *
 * The `gpt-4o-audio-preview` model can also be used to
 * [generate audio](/docs/guides/audio). To request that this model generate
 * both text and audio responses, you can use:
 *
 * `["text", "audio"]`
 */
export type ChatCompletionInputResponseModality = "text" | "audio";
/**
 * Configuration for a [Predicted Output](/docs/guides/predicted-outputs),
 * which can greatly improve response times when large parts of the model
 * response are known ahead of time. This is most common when you are
 * regenerating a file with only minor changes to most of the content.
 *
 *
 * Static predicted output content, such as the content of a text file that is
 * being regenerated.
 */
export interface ChatCompletionInputPredictionContent {
	/**
	 * The content that should be matched when generating a model response.
	 * If generated tokens would match this content, the entire model response
	 * can be returned much more quickly.
	 */
	content: PredictionContent;
	/**
	 * The type of the predicted content you want to provide. This type is
	 * currently always `content`.
	 */
	type: "content";
	[property: string]: unknown;
}
/**
 * The contents of the system message.
 *
 * The contents of the tool message.
 */
export type PredictionContent = ChatCompletionInputRequest[] | string;
/**
 * An array of content parts with a defined type. For developer messages, only type `text`
 * is supported.
 *
 * Learn about [text inputs](/docs/guides/text-generation).
 *
 *
 * An array of content parts with a defined type. Supported options differ based on the
 * [model](/docs/models) being used to generate the response. Can contain text inputs.
 *
 * An array of content parts with a defined type. For system messages, only type `text` is
 * supported.
 *
 * An array of content parts with a defined type. For tool messages, only type `text` is
 * supported.
 */
export interface ChatCompletionInputRequest {
	/**
	 * The text content.
	 */
	text: string;
	/**
	 * The type of the content part.
	 */
	type: "text";
	[property: string]: unknown;
}
/**
 * The type of the content part.
 */
/**
 * The type of the predicted content you want to provide. This type is
 * currently always `content`.
 */
/**
 * **o-series models only**
 *
 * Constrains effort on reasoning for
 * [reasoning models](https://platform.openai.com/docs/guides/reasoning).
 * Currently supported values are `low`, `medium`, and `high`. Reducing
 * reasoning effort can result in faster responses and fewer tokens used
 * on reasoning in a response.
 *
 *
 * High level guidance for the amount of context window space to use for the
 * search. One of `low`, `medium`, or `high`. `medium` is the default.
 */
export type ReasoningEffortEnum = "low" | "medium" | "high";
/**
 * An object specifying the format that the model must output.
 *
 * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
 * Structured Outputs which ensures the model will match your supplied JSON
 * schema. Learn more in the [Structured Outputs
 * guide](/docs/guides/structured-outputs).
 *
 * Setting to `{ "type": "json_object" }` enables the older JSON mode, which
 * ensures the message the model generates is valid JSON. Using `json_schema`
 * is preferred for models that support it.
 *
 *
 * Default response format. Used to generate text responses.
 *
 *
 * JSON Schema response format. Used to generate structured JSON responses.
 * Learn more about [Structured Outputs](/docs/guides/structured-outputs).
 *
 *
 * JSON object response format. An older method of generating JSON responses.
 * Using `json_schema` is recommended for models that support it. Note that the
 * model will not generate JSON without a system or user message instructing it
 * to do so.
 */
export interface ChatCompletionInputResponseFormat {
	/**
	 * Structured Outputs configuration options, including a JSON Schema.
	 */
	json_schema?: JSONSchema;
	/**
	 * The type of response format being defined. Always `text`.
	 *
	 * The type of response format being defined. Always `json_schema`.
	 *
	 * The type of response format being defined. Always `json_object`.
	 */
	type: ResponseFormatType;
	[property: string]: unknown;
}
/**
 * Structured Outputs configuration options, including a JSON Schema.
 */
export interface JSONSchema {
	/**
	 * A description of what the response format is for, used by the model to
	 * determine how to respond in the format.
	 */
	description?: string;
	/**
	 * The name of the response format. Must be a-z, A-Z, 0-9, or contain
	 * underscores and dashes, with a maximum length of 64.
	 */
	name: string;
	schema?: {
		[key: string]: unknown;
	};
	/**
	 * Whether to enable strict schema adherence when generating the output.
	 * If set to true, the model will always follow the exact schema defined
	 * in the `schema` field. Only a subset of JSON Schema is supported when
	 * `strict` is `true`. To learn more, read the [Structured Outputs
	 * guide](/docs/guides/structured-outputs).
	 */
	strict?: boolean;
	[property: string]: unknown;
}
/**
 * The type of response format being defined. Always `text`.
 *
 * The type of response format being defined. Always `json_schema`.
 *
 * The type of response format being defined. Always `json_object`.
 */
export type ResponseFormatType = "text" | "json_schema" | "json_object";
/**
 * Specifies the latency tier to use for processing the request. This parameter is relevant
 * for customers subscribed to the scale tier service:
 * - If set to 'auto', and the Project is Scale tier enabled, the system
 * will utilize scale tier credits until they are exhausted.
 * - If set to 'auto', and the Project is not Scale tier enabled, the request will be
 * processed using the default service tier with a lower uptime SLA and no latency
 * guarentee.
 * - If set to 'default', the request will be processed using the default service tier with
 * a lower uptime SLA and no latency guarentee.
 * - When not set, the default behavior is 'auto'.
 *
 * When this parameter is set, the response body will include the `service_tier` utilized.
 */
export type ChatCompletionInputServiceTier = "auto" | "default";
/**
 * Up to 4 sequences where the API will stop generating further tokens. The
 * returned text will not contain the stop sequence.
 */
export type ChatCompletionInputStopConfiguration = string[] | string;
/**
 * Options for streaming response. Only set this when you set `stream: true`.
 */
export interface ChatCompletionInputStreamOptions {
	/**
	 * If set, an additional chunk will be streamed before the `data: [DONE]`
	 * message. The `usage` field on this chunk shows the token usage statistics
	 * for the entire request, and the `choices` field will always be an empty
	 * array.
	 *
	 * All other chunks will also include a `usage` field, but with a null
	 * value. **NOTE:** If the stream is interrupted, you may not receive the
	 * final usage chunk which contains the total token usage for the request.
	 */
	include_usage?: boolean;
	[property: string]: unknown;
}
/**
 * Controls which (if any) tool is called by the model.
 * `none` means the model will not call any tool and instead generates a message.
 * `auto` means the model can pick between generating a message or calling one or more
 * tools.
 * `required` means the model must call one or more tools.
 * Specifying a particular tool via `{"type": "function", "function": {"name":
 * "my_function"}}` forces the model to call that tool.
 *
 * `none` is the default when no tools are present. `auto` is the default if tools are
 * present.
 */
export type ChatCompletionInputToolChoiceOption =
	| ChatCompletionInputToolChoiceOptionEnum
	| ChatCompletionInputNamedToolChoice;
/**
 * `none` means the model will not call any tool and instead generates a message. `auto`
 * means the model can pick between generating a message or calling one or more tools.
 * `required` means the model must call one or more tools.
 */
export type ChatCompletionInputToolChoiceOptionEnum = "none" | "auto" | "required";
/**
 * Specifies a tool the model should use. Use to force the model to call a specific function.
 */
export interface ChatCompletionInputNamedToolChoice {
	function: ChatCompletionInputNamedToolChoiceFunction;
	/**
	 * The type of the tool. Currently, only `function` is supported.
	 */
	type: "function";
	[property: string]: unknown;
}
export interface ChatCompletionInputNamedToolChoiceFunction {
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
export interface ChatCompletionInputTool {
	function: ChatCompletionInputFunctionObject;
	/**
	 * The type of the tool. Currently, only `function` is supported.
	 */
	type: "function";
	[property: string]: unknown;
}
export interface ChatCompletionInputFunctionObject {
	/**
	 * A description of what the function does, used by the model to choose when and how to call
	 * the function.
	 */
	description?: string;
	/**
	 * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
	 * dashes, with a maximum length of 64.
	 */
	name: string;
	parameters?: {
		[key: string]: unknown;
	};
	/**
	 * Whether to enable strict schema adherence when generating the function call. If set to
	 * true, the model will follow the exact schema defined in the `parameters` field. Only a
	 * subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured
	 * Outputs in the [function calling guide](docs/guides/function-calling).
	 */
	strict?: boolean;
	[property: string]: unknown;
}
/**
 * This tool searches the web for relevant results to use in a response.
 * Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
 */
export interface WebSearch {
	search_context_size?: ReasoningEffortEnum;
	/**
	 * Approximate location parameters for the search.
	 */
	user_location?: ChatCompletionInputUserLocation;
	[property: string]: unknown;
}
/**
 * Approximate location parameters for the search.
 */
export interface ChatCompletionInputUserLocation {
	approximate: ChatCompletionInputWebSearchLocation;
	/**
	 * The type of location approximation. Always `approximate`.
	 */
	type: "approximate";
	[property: string]: unknown;
}
/**
 * Approximate location parameters for the search.
 */
export interface ChatCompletionInputWebSearchLocation {
	/**
	 * Free text input for the city of the user, e.g. `San Francisco`.
	 */
	city?: string;
	/**
	 * The two-letter
	 * [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user,
	 * e.g. `US`.
	 */
	country?: string;
	/**
	 * Free text input for the region of the user, e.g. `California`.
	 */
	region?: string;
	/**
	 * The [IANA timezone](https://timeapi.io/documentation/iana-timezones)
	 * of the user, e.g. `America/Los_Angeles`.
	 */
	timezone?: string;
	[property: string]: unknown;
}
/**
 * The type of location approximation. Always `approximate`.
 */
/**
 * Chat Completion Output.
 *
 * Auto-generated from OAI specs.
 * For more details, check out
 * https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
 */
export interface ChatCompletionOutput {
	/**
	 * A list of chat completion choices. Can be more than one if `n` is greater than 1.
	 */
	choices: ChatCompletionOutputChoicesItem[];
	/**
	 * The Unix timestamp (in seconds) of when the chat completion was created.
	 */
	created: number;
	/**
	 * A unique identifier for the chat completion.
	 */
	id: string;
	/**
	 * The model used for the chat completion.
	 */
	model: string;
	/**
	 * The object type, which is always `chat.completion`.
	 */
	object: "chat.completion";
	/**
	 * The service tier used for processing the request.
	 */
	service_tier?: ChatCompletionOutputServiceTier;
	/**
	 * This fingerprint represents the backend configuration that the model runs with.
	 *
	 * Can be used in conjunction with the `seed` request parameter to understand when backend
	 * changes have been made that might impact determinism.
	 */
	system_fingerprint?: string;
	usage?: ChatCompletionOutputCompletionUsage;
	[property: string]: unknown;
}
export interface ChatCompletionOutputChoicesItem {
	/**
	 * The reason the model stopped generating tokens. This will be `stop` if the model hit a
	 * natural stop point or a provided stop sequence,
	 * `length` if the maximum number of tokens specified in the request was reached,
	 * `content_filter` if content was omitted due to a flag from our content filters,
	 * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
	 * called a function.
	 */
	finish_reason: FinishReason;
	/**
	 * The index of the choice in the list of choices.
	 */
	index: number;
	/**
	 * Log probability information for the choice.
	 */
	logprobs: ChatCompletionOutputLogprobs;
	message: ChatCompletionOutputResponseMessage;
	[property: string]: unknown;
}
/**
 * The reason the model stopped generating tokens. This will be `stop` if the model hit a
 * natural stop point or a provided stop sequence,
 * `length` if the maximum number of tokens specified in the request was reached,
 * `content_filter` if content was omitted due to a flag from our content filters,
 * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
 * called a function.
 */
export type FinishReason = "stop" | "length" | "tool_calls" | "content_filter" | "function_call";
/**
 * Log probability information for the choice.
 */
export interface ChatCompletionOutputLogprobs {
	/**
	 * A list of message content tokens with log probability information.
	 */
	content: ChatCompletionOutputTokenLogprob[];
	/**
	 * A list of message refusal tokens with log probability information.
	 */
	refusal: ChatCompletionOutputTokenLogprob[];
	[property: string]: unknown;
}
export interface ChatCompletionOutputTokenLogprob {
	/**
	 * A list of integers representing the UTF-8 bytes representation of the token. Useful in
	 * instances where characters are represented by multiple tokens and their byte
	 * representations must be combined to generate the correct text representation. Can be
	 * `null` if there is no bytes representation for the token.
	 */
	bytes: number[];
	/**
	 * The log probability of this token, if it is within the top 20 most likely tokens.
	 * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
	 */
	logprob: number;
	/**
	 * The token.
	 */
	token: string;
	/**
	 * List of the most likely tokens and their log probability, at this token position. In rare
	 * cases, there may be fewer than the number of requested `top_logprobs` returned.
	 */
	top_logprobs: ChatCompletionOutputTokenLogprobTopLogprobsItem[];
	[property: string]: unknown;
}
export interface ChatCompletionOutputTokenLogprobTopLogprobsItem {
	/**
	 * A list of integers representing the UTF-8 bytes representation of the token. Useful in
	 * instances where characters are represented by multiple tokens and their byte
	 * representations must be combined to generate the correct text representation. Can be
	 * `null` if there is no bytes representation for the token.
	 */
	bytes: number[];
	/**
	 * The log probability of this token, if it is within the top 20 most likely tokens.
	 * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
	 */
	logprob: number;
	/**
	 * The token.
	 */
	token: string;
	[property: string]: unknown;
}
/**
 * A chat completion message generated by the model.
 */
export interface ChatCompletionOutputResponseMessage {
	/**
	 * Annotations for the message, when applicable, as when using the
	 * [web search tool](/docs/guides/tools-web-search?api-mode=chat).
	 */
	annotations?: ChatCompletionOutputResponseMessageAnnotationsItem[];
	/**
	 * If the audio output modality is requested, this object contains data
	 * about the audio response from the model. [Learn more](/docs/guides/audio).
	 */
	audio?: ChatCompletionOutputResponseMessageAudio;
	/**
	 * The contents of the message.
	 */
	content: string;
	/**
	 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
	 * be called, as generated by the model.
	 */
	function_call?: ChatCompletionOutputResponseMessageFunctionCall;
	/**
	 * The refusal message generated by the model.
	 */
	refusal: string;
	/**
	 * The role of the author of this message.
	 */
	role: "assistant";
	tool_calls?: ChatCompletionOutputMessageToolCall[];
	[property: string]: unknown;
}
/**
 * A URL citation when using web search.
 */
export interface ChatCompletionOutputResponseMessageAnnotationsItem {
	/**
	 * The type of the URL citation. Always `url_citation`.
	 */
	type: "url_citation";
	/**
	 * A URL citation when using web search.
	 */
	url_citation: ChatCompletionOutputResponseMessageURLCitation;
	[property: string]: unknown;
}
/**
 * The type of the URL citation. Always `url_citation`.
 */
/**
 * A URL citation when using web search.
 */
export interface ChatCompletionOutputResponseMessageURLCitation {
	/**
	 * The index of the last character of the URL citation in the message.
	 */
	end_index: number;
	/**
	 * The index of the first character of the URL citation in the message.
	 */
	start_index: number;
	/**
	 * The title of the web resource.
	 */
	title: string;
	/**
	 * The URL of the web resource.
	 */
	url: string;
	[property: string]: unknown;
}
/**
 * If the audio output modality is requested, this object contains data
 * about the audio response from the model. [Learn more](/docs/guides/audio).
 */
export interface ChatCompletionOutputResponseMessageAudio {
	/**
	 * Base64 encoded audio bytes generated by the model, in the format
	 * specified in the request.
	 */
	data: string;
	/**
	 * The Unix timestamp (in seconds) for when this audio response will
	 * no longer be accessible on the server for use in multi-turn
	 * conversations.
	 */
	expires_at: number;
	/**
	 * Unique identifier for this audio response.
	 */
	id: string;
	/**
	 * Transcript of the audio generated by the model.
	 */
	transcript: string;
	[property: string]: unknown;
}
/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
 * be called, as generated by the model.
 */
export interface ChatCompletionOutputResponseMessageFunctionCall {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments: string;
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
/**
 * The role of the author of this message.
 */
/**
 * The tool calls generated by the model, such as function calls.
 */
export interface ChatCompletionOutputMessageToolCall {
	/**
	 * The function that the model called.
	 */
	function: ChatCompletionOutputMessageToolCallFunction;
	/**
	 * The ID of the tool call.
	 */
	id: string;
	/**
	 * The type of the tool. Currently, only `function` is supported.
	 */
	type: "function";
	[property: string]: unknown;
}
/**
 * The function that the model called.
 */
export interface ChatCompletionOutputMessageToolCallFunction {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments: string;
	/**
	 * The name of the function to call.
	 */
	name: string;
	[property: string]: unknown;
}
/**
 * The object type, which is always `chat.completion`.
 */
/**
 * The service tier used for processing the request.
 */
export type ChatCompletionOutputServiceTier = "scale" | "default";
/**
 * Usage statistics for the completion request.
 */
export interface ChatCompletionOutputCompletionUsage {
	/**
	 * Number of tokens in the generated completion.
	 */
	completion_tokens: number;
	/**
	 * Breakdown of tokens used in a completion.
	 */
	completion_tokens_details?: ChatCompletionOutputCompletionUsageCompletionTokensDetails;
	/**
	 * Number of tokens in the prompt.
	 */
	prompt_tokens: number;
	/**
	 * Breakdown of tokens used in the prompt.
	 */
	prompt_tokens_details?: ChatCompletionOutputCompletionUsagePromptTokensDetails;
	/**
	 * Total number of tokens used in the request (prompt + completion).
	 */
	total_tokens: number;
	[property: string]: unknown;
}
/**
 * Breakdown of tokens used in a completion.
 */
export interface ChatCompletionOutputCompletionUsageCompletionTokensDetails {
	/**
	 * When using Predicted Outputs, the number of tokens in the
	 * prediction that appeared in the completion.
	 */
	accepted_prediction_tokens?: number;
	/**
	 * Audio input tokens generated by the model.
	 */
	audio_tokens?: number;
	/**
	 * Tokens generated by the model for reasoning.
	 */
	reasoning_tokens?: number;
	/**
	 * When using Predicted Outputs, the number of tokens in the
	 * prediction that did not appear in the completion. However, like
	 * reasoning tokens, these tokens are still counted in the total
	 * completion tokens for purposes of billing, output, and context window
	 * limits.
	 */
	rejected_prediction_tokens?: number;
	[property: string]: unknown;
}
/**
 * Breakdown of tokens used in the prompt.
 */
export interface ChatCompletionOutputCompletionUsagePromptTokensDetails {
	/**
	 * Audio input tokens present in the prompt.
	 */
	audio_tokens?: number;
	/**
	 * Cached tokens present in the prompt.
	 */
	cached_tokens?: number;
	[property: string]: unknown;
}
/**
 * Chat Completion Stream Output.
 *
 * Auto-generated from OAI specs.
 * For more details, check out
 * https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-oai-import.ts.
 */
export interface ChatCompletionStreamOutput {
	/**
	 * A list of chat completion choices. Can contain more than one elements if `n` is greater
	 * than 1. Can also be empty for the
	 * last chunk if you set `stream_options: {"include_usage": true}`.
	 */
	choices: ChatCompletionStreamOutputChoicesItem[];
	/**
	 * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has
	 * the same timestamp.
	 */
	created: number;
	/**
	 * A unique identifier for the chat completion. Each chunk has the same ID.
	 */
	id: string;
	/**
	 * The model to generate the completion.
	 */
	model: string;
	/**
	 * The object type, which is always `chat.completion.chunk`.
	 */
	object: "chat.completion.chunk";
	/**
	 * The service tier used for processing the request.
	 */
	service_tier?: ChatCompletionOutputServiceTier;
	/**
	 * This fingerprint represents the backend configuration that the model runs with.
	 * Can be used in conjunction with the `seed` request parameter to understand when backend
	 * changes have been made that might impact determinism.
	 */
	system_fingerprint?: string;
	/**
	 * An optional field that will only be present when you set
	 * `stream_options: {"include_usage": true}` in your request. When present, it
	 * contains a null value **except for the last chunk** which contains the
	 * token usage statistics for the entire request.
	 *
	 * **NOTE:** If the stream is interrupted or cancelled, you may not
	 * receive the final usage chunk which contains the total token usage for
	 * the request.
	 */
	usage?: ChatCompletionStreamOutputCompletionUsage;
	[property: string]: unknown;
}
export interface ChatCompletionStreamOutputChoicesItem {
	delta: ChatCompletionStreamOutputStreamResponseDelta;
	/**
	 * The reason the model stopped generating tokens. This will be `stop` if the model hit a
	 * natural stop point or a provided stop sequence,
	 * `length` if the maximum number of tokens specified in the request was reached,
	 * `content_filter` if content was omitted due to a flag from our content filters,
	 * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
	 * called a function.
	 */
	finish_reason: FinishReason;
	/**
	 * The index of the choice in the list of choices.
	 */
	index: number;
	/**
	 * Log probability information for the choice.
	 */
	logprobs?: ChatCompletionStreamOutputLogprobs;
	[property: string]: unknown;
}
/**
 * A chat completion delta generated by streamed model responses.
 */
export interface ChatCompletionStreamOutputStreamResponseDelta {
	/**
	 * The contents of the chunk message.
	 */
	content?: string;
	/**
	 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
	 * be called, as generated by the model.
	 */
	function_call?: ChatCompletionStreamOutputStreamResponseDeltaFunctionCall;
	/**
	 * The refusal message generated by the model.
	 */
	refusal?: string;
	/**
	 * The role of the author of this message.
	 */
	role?: DeltaRole;
	tool_calls?: ChatCompletionStreamOutputMessageToolCallChunk[];
	[property: string]: unknown;
}
/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
 * be called, as generated by the model.
 */
export interface ChatCompletionStreamOutputStreamResponseDeltaFunctionCall {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments?: string;
	/**
	 * The name of the function to call.
	 */
	name?: string;
	[property: string]: unknown;
}
/**
 * The role of the author of this message.
 */
export type DeltaRole = "developer" | "system" | "user" | "assistant" | "tool";
export interface ChatCompletionStreamOutputMessageToolCallChunk {
	function?: ChatCompletionStreamOutputMessageToolCallChunkFunction;
	/**
	 * The ID of the tool call.
	 */
	id?: string;
	index: number;
	/**
	 * The type of the tool. Currently, only `function` is supported.
	 */
	type?: "function";
	[property: string]: unknown;
}
export interface ChatCompletionStreamOutputMessageToolCallChunkFunction {
	/**
	 * The arguments to call the function with, as generated by the model in JSON format. Note
	 * that the model does not always generate valid JSON, and may hallucinate parameters not
	 * defined by your function schema. Validate the arguments in your code before calling your
	 * function.
	 */
	arguments?: string;
	/**
	 * The name of the function to call.
	 */
	name?: string;
	[property: string]: unknown;
}
/**
 * Log probability information for the choice.
 */
export interface ChatCompletionStreamOutputLogprobs {
	/**
	 * A list of message content tokens with log probability information.
	 */
	content: ChatCompletionStreamOutputTokenLogprob[];
	/**
	 * A list of message refusal tokens with log probability information.
	 */
	refusal: ChatCompletionStreamOutputTokenLogprob[];
	[property: string]: unknown;
}
export interface ChatCompletionStreamOutputTokenLogprob {
	/**
	 * A list of integers representing the UTF-8 bytes representation of the token. Useful in
	 * instances where characters are represented by multiple tokens and their byte
	 * representations must be combined to generate the correct text representation. Can be
	 * `null` if there is no bytes representation for the token.
	 */
	bytes: number[];
	/**
	 * The log probability of this token, if it is within the top 20 most likely tokens.
	 * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
	 */
	logprob: number;
	/**
	 * The token.
	 */
	token: string;
	/**
	 * List of the most likely tokens and their log probability, at this token position. In rare
	 * cases, there may be fewer than the number of requested `top_logprobs` returned.
	 */
	top_logprobs: ChatCompletionStreamOutputTokenLogprobTopLogprobsItem[];
	[property: string]: unknown;
}
export interface ChatCompletionStreamOutputTokenLogprobTopLogprobsItem {
	/**
	 * A list of integers representing the UTF-8 bytes representation of the token. Useful in
	 * instances where characters are represented by multiple tokens and their byte
	 * representations must be combined to generate the correct text representation. Can be
	 * `null` if there is no bytes representation for the token.
	 */
	bytes: number[];
	/**
	 * The log probability of this token, if it is within the top 20 most likely tokens.
	 * Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
	 */
	logprob: number;
	/**
	 * The token.
	 */
	token: string;
	[property: string]: unknown;
}
/**
 * The object type, which is always `chat.completion.chunk`.
 */
/**
 * An optional field that will only be present when you set
 * `stream_options: {"include_usage": true}` in your request. When present, it
 * contains a null value **except for the last chunk** which contains the
 * token usage statistics for the entire request.
 *
 * **NOTE:** If the stream is interrupted or cancelled, you may not
 * receive the final usage chunk which contains the total token usage for
 * the request.
 *
 *
 * Usage statistics for the completion request.
 */
export interface ChatCompletionStreamOutputCompletionUsage {
	/**
	 * Number of tokens in the generated completion.
	 */
	completion_tokens: number;
	/**
	 * Breakdown of tokens used in a completion.
	 */
	completion_tokens_details?: ChatCompletionStreamOutputCompletionUsageCompletionTokensDetails;
	/**
	 * Number of tokens in the prompt.
	 */
	prompt_tokens: number;
	/**
	 * Breakdown of tokens used in the prompt.
	 */
	prompt_tokens_details?: ChatCompletionStreamOutputCompletionUsagePromptTokensDetails;
	/**
	 * Total number of tokens used in the request (prompt + completion).
	 */
	total_tokens: number;
	[property: string]: unknown;
}
/**
 * Breakdown of tokens used in a completion.
 */
export interface ChatCompletionStreamOutputCompletionUsageCompletionTokensDetails {
	/**
	 * When using Predicted Outputs, the number of tokens in the
	 * prediction that appeared in the completion.
	 */
	accepted_prediction_tokens?: number;
	/**
	 * Audio input tokens generated by the model.
	 */
	audio_tokens?: number;
	/**
	 * Tokens generated by the model for reasoning.
	 */
	reasoning_tokens?: number;
	/**
	 * When using Predicted Outputs, the number of tokens in the
	 * prediction that did not appear in the completion. However, like
	 * reasoning tokens, these tokens are still counted in the total
	 * completion tokens for purposes of billing, output, and context window
	 * limits.
	 */
	rejected_prediction_tokens?: number;
	[property: string]: unknown;
}
/**
 * Breakdown of tokens used in the prompt.
 */
export interface ChatCompletionStreamOutputCompletionUsagePromptTokensDetails {
	/**
	 * Audio input tokens present in the prompt.
	 */
	audio_tokens?: number;
	/**
	 * Cached tokens present in the prompt.
	 */
	cached_tokens?: number;
	[property: string]: unknown;
}
