{
	"id": "http://huggingface.co/inference/schemas/text-generation/input.json",
	"$schema": "http://json-schema.org/draft-06/schema#",
	"description": "Inputs for Text Generation inference",
	"type": "object",
	"properties": {
		"inputs": {
			"description": "The text to initialize generation with",
			"anyOf": [
				{
					"type": "string"
				},
				{
					"type": "array",
					"items": {
						"type": "string"
					}
				}
			]
		},
		"parameters": {
			"description": "Additional inference parameters",
			"$ref": "#/definitions/TextGenerationParameters"
		}
	},
	"definitions": {
		"TextGenerationParameters": {
			"description": "Additional inference parameters for Text Generation",
			"type": "object",
			"properties": {
				"doSample": {
					"type": "boolean",
					"description": "Whether to use logit sampling (true) or greedy search (false)."
				},
				"maxNewTokens": {
					"type": "integer",
					"description": "Maximum number of generated tokens."
				},
				"repetitionPenalty": {
					"type": "number",
					"description": "The parameter for repetition penalty. A value of 1.0 means no penalty. See [this paper](https://hf.co/papers/1909.05858) for more details."
				},
				"returnFullText": {
					"type": "boolean",
					"description": "Whether to prepend the prompt to the generated text."
				},
				"stopSequences": {
					"type": "array",
					"items": {
						"type": "string"
					},
					"description": "Stop generating tokens if a member of `stop_sequences` is generated."
				},
				"temperature": {
					"type": "number",
					"description": "The value used to modulate the logits distribution."
				},
				"topK": {
					"type": "integer",
					"description": "The number of highest probability vocabulary tokens to keep for top-k-filtering."
				},
				"topP": {
					"type": "number",
					"description": "If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation."
				},
				"truncate": {
					"type": "integer",
					"description": "Truncate input tokens to the given size."
				},
				"typicalP": {
					"type": "number",
					"description": "Typical Decoding mass. See [Typical Decoding for Natural Language Generation](https://hf.co/papers/2202.00666) for more information"
				},
				"watermark": {
					"type": "boolean",
					"description": "Watermarking with [A Watermark for Large Language Models](https://hf.co/papers/2301.10226)"
				}
			}
		}
	},
	"required": ["inputs"]
}
